{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31f2ca9-a17e-43c0-83ba-279d1e217ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 17:53:34.344300: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-31 17:53:34.344610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-31 17:53:34.423339: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-31 17:53:34.578695: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-31 17:53:36.080420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-12-31 17:53:40.099752: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-31 17:53:40.393121: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-31 17:53:40.393241: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from choice_learn.data import ChoiceDataset\n",
    "from choice_learn.models import SimpleMNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc1ddd4-1f0c-4fbb-bce2-7850050fc3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device = '/GPU:0'\n",
    "    print(f\"Use GPU: {tf.config.list_physical_devices('GPU')[0]}\")\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "    print(\"Use CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca331fed-06de-4a66-adb6-d98e65eb9d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Q3/DeepHalo-tf/Final/DeepHalo\n"
     ]
    }
   ],
   "source": [
    "project_root = os.path.abspath(os.getcwd())\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Make sure you are in the package's main directory\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf65669-631e-4380-965e-ffbef058b981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DeepHaloChoiceModel package v1.0.0\n"
     ]
    }
   ],
   "source": [
    "from DeepHalo import DeepHaloChoiceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17cff1-a888-487d-a5c4-e1ad3967f675",
   "metadata": {},
   "source": [
    "# Load Data from ChoiceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbfbb76b-f57b-4bfe-b01b-da7531fa5929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%=====================================================================%\n",
      "%%% Summary of the dataset:\n",
      "%=====================================================================%\n",
      "Number of items: 3\n",
      "Number of choices: 10719\n",
      "%=====================================================================%\n",
      " No Shared Features by Choice registered\n",
      "\n",
      "\n",
      " Items Features by Choice:\n",
      "2 items features \n",
      " with names: (['CO', 'TT'],)\n",
      "%=====================================================================%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from choice_learn.datasets import load_swissmetro\n",
    "from choice_learn.datasets import load_modecanada\n",
    "\n",
    "dataset_name = 'swiss'\n",
    "\n",
    "def choose_dataset(dataset_name):\n",
    "    if dataset_name == 'swiss':\n",
    "\n",
    "        # Load SwissMetro (transportation) data\n",
    "        swiss_df = load_swissmetro(as_frame=True)\n",
    "        \n",
    "        kept_columns = [\n",
    "            \"PURPOSE\", \"AGE\",\n",
    "            \"CAR_AV\", \"TRAIN_AV\", \"SM_AV\",\n",
    "            \"CAR_TT\", \"TRAIN_TT\", \"SM_TT\",\n",
    "            \"CAR_CO\", \"TRAIN_CO\", \"SM_CO\",\n",
    "            \"CHOICE\",\n",
    "        ]\n",
    "        swiss_df = swiss_df[kept_columns]\n",
    "\n",
    "        # Build ChoiceDataset in wide format\n",
    "        dataset = ChoiceDataset.from_single_wide_df(\n",
    "            df=swiss_df,\n",
    "            items_id=[\"TRAIN\", \"SM\", \"CAR\"],   # names used in *_AV, *_TT, *_CO columns\n",
    "            choices_column=\"CHOICE\",\n",
    "            choice_format=\"items_index\",       # CHOICE is 0/1/2 index of TRAIN/SM/CAR\n",
    "            shared_features_columns=None,\n",
    "            # shared_features_columns=[\"GROUP\", \"SURVEY\", \"SP\", \"ID\", \"PURPOSE\", \"FIRST\", \n",
    "            #                         \"TICKET\", \"WHO\", \"LUGGAGE\", \"AGE\", \"MALE\", \"INCOME\", \n",
    "            #                         \"GA\", \"ORIGIN\", \"DEST\"],\n",
    "            # items_features_suffixes=[\"CO\", \"TT\", \"HE\", \"SEATS\"],\n",
    "            items_features_suffixes=[\"CO\", \"TT\"],    # features: *_CO, *_TT\n",
    "            available_items_suffix=\"AV\",\n",
    "            delimiter=\"_\",\n",
    "        )\n",
    "\n",
    "    elif dataset_name == 'canada':\n",
    "\n",
    "        # Load ModeCanada (transportation) data\n",
    "        canada_transport_df = load_modecanada(as_frame=True)\n",
    "    \n",
    "        dataset = ChoiceDataset.from_single_long_df(\n",
    "        df=canada_transport_df,\n",
    "        choices_column=\"choice\",\n",
    "        items_id_column=\"alt\",\n",
    "        choices_id_column=\"case\",\n",
    "        shared_features_columns=None,\n",
    "        items_features_columns=[\"cost\", \"freq\", \"ovt\", \"ivt\"],\n",
    "        choice_format=\"one_zero\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\") \n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = choose_dataset(dataset_name)\n",
    "print(dataset.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0576d3c4-ce98-47be-bd48-0e797b51800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_choice_dataset(dataset, train_idx, test_idx): \n",
    "    \n",
    "    # Split ChoiceDataset into train/test ChoiceDatasets preserving structure.\n",
    "\n",
    "  \n",
    "    # Extract train/test subsets\n",
    "    train_choices = dataset.choices[train_idx]\n",
    "    test_choices = dataset.choices[test_idx]\n",
    "    \n",
    "    # Slice items_features_by_choice (list of tuples)\n",
    "    train_items_features = [dataset.items_features_by_choice[0][i] for i in train_idx]\n",
    "    test_items_features = [dataset.items_features_by_choice[0][i] for i in test_idx]\n",
    "    \n",
    "    # Slice availability matrix\n",
    "    train_avail = dataset.available_items_by_choice[train_idx]\n",
    "    test_avail = dataset.available_items_by_choice[test_idx]\n",
    "    \n",
    "    # Create new ChoiceDatasets \n",
    "    train_dataset = ChoiceDataset(\n",
    "        items_features_by_choice=train_items_features,\n",
    "        choices=train_choices,\n",
    "        available_items_by_choice=train_avail,\n",
    "        shared_features_by_choice_names=dataset.shared_features_by_choice_names,\n",
    "        items_features_by_choice_names=dataset.items_features_by_choice_names[0]\n",
    "    )\n",
    "    \n",
    "    test_dataset = ChoiceDataset(\n",
    "        items_features_by_choice=test_items_features,\n",
    "        choices=test_choices,\n",
    "        available_items_by_choice=test_avail,\n",
    "        shared_features_by_choice_names=dataset.shared_features_by_choice_names,\n",
    "        items_features_by_choice_names=dataset.items_features_by_choice_names[0]\n",
    "    )\n",
    "    \n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e776ee99-bf71-469a-8ffc-36045e2d2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choice_accuracy(pred_probs, true_labels, available_items_by_choice=None):\n",
    "    \n",
    "    # Compute choice prediction accuracy from predicted probabilities vs true labels.\n",
    "\n",
    "    \n",
    "    n_choices = len(true_labels)\n",
    "    correct = 0\n",
    "    \n",
    "    # Handle availability (mask unavailable alternatives)\n",
    "    if available_items_by_choice is not None:\n",
    "        # Only consider available alternatives for prediction\n",
    "        masked_probs = pred_probs.copy()\n",
    "        masked_probs[available_items_by_choice == 0] = -np.inf  # Impossible choices\n",
    "        pred_indices = np.argmax(masked_probs, axis=1)\n",
    "    else:\n",
    "        pred_indices = np.argmax(pred_probs, axis=1)\n",
    "    \n",
    "    # Count correct predictions\n",
    "    correct = np.sum(pred_indices == true_labels)\n",
    "    accuracy = correct / n_choices\n",
    "    \n",
    "    # Per-alternative accuracy\n",
    "    per_alt_acc = {}\n",
    "    for alt in range(pred_probs.shape[1]):\n",
    "        alt_correct = np.sum((pred_indices == alt) & (true_labels == alt))\n",
    "        alt_total = np.sum(true_labels == alt)\n",
    "        per_alt_acc[f'Alt{alt}'] = alt_correct / alt_total if alt_total > 0 else 0\n",
    "    \n",
    "    return accuracy, {\n",
    "        'correct': correct,\n",
    "        'total': n_choices,\n",
    "        'predicted_indices': pred_indices,\n",
    "        'per_alternative': per_alt_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c432e3-2ac5-4b25-805a-dc89d566a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=0.2; random_state=42; stratify=True;\n",
    "n_choices = len(dataset.choices)\n",
    "    \n",
    "# Get choice indices stratified by chosen alternative\n",
    "choice_indices = np.arange(n_choices)\n",
    "\n",
    "if stratify:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        choice_indices, test_size=test_size, \n",
    "        random_state=random_state, stratify=dataset.choices\n",
    "    )\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        choice_indices, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = train_test_split_choice_dataset(dataset, train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242a5bb-13c9-40fd-bdd0-a6ba1a2ccd2c",
   "metadata": {},
   "source": [
    "# Choice Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ea55e1-5653-4071-91c5-b600cb6855a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.models.simple_mnl import SimpleMNL\n",
    "from choice_learn.models import ConditionalLogit\n",
    "from choice_learn.models.halo_mnl import LowRankHaloMNL, HaloMNL\n",
    "from choice_learn.models import RUMnet\n",
    "from choice_learn.models import NestedLogit\n",
    "from choice_learn.models.latent_class_mnl import LatentClassSimpleMNL\n",
    "from choice_learn.models import ResLogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61362b18-de57-4e0d-88c0-1ec40f1e9265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name):\n",
    "    \n",
    "    if model_name == \"SimpleMNL\":\n",
    "        model = SimpleMNL(intercept=\"item\")  \n",
    "        \n",
    "    elif model_name == \"ConditionalLogit\":\n",
    "        model = ConditionalLogit(optimizer=\"lbfgs\")\n",
    "\n",
    "        # Intercept for train & sm\n",
    "        model.add_coefficients(feature_name=\"intercept\", items_indexes=[0, 1])\n",
    "        \n",
    "        # beta_co for all items\n",
    "        model.add_coefficients(feature_name=\"CO\",\n",
    "                                     items_indexes=[0, 1, 2])\n",
    "        \n",
    "        # beta TT for car\n",
    "        model.add_coefficients(feature_name=\"TT\",\n",
    "                                     items_indexes=[2],\n",
    "                    \t\t\t     coefficient_name=\"beta_tt_car\")\n",
    "        \n",
    "        # betas TT shared by train and sm\n",
    "        model.add_shared_coefficient(feature_name=\"TT\",\n",
    "                                           items_indexes=[0, 1])\n",
    "    elif model_name == \"HaloMNL\":\n",
    "        model = HaloMNL(intercept=\"item\", optimizer=\"lbfgs\")\n",
    "\n",
    "    elif model_name == \"LowRankHaloMNL\": \n",
    "        model = LowRankHaloMNL(halo_latent_dim=2, intercept=None)\n",
    "\n",
    "    elif model_name == \"RUMnet\": \n",
    "        model_args = {\n",
    "                \"num_products_features\": 6,\n",
    "                \"num_customer_features\": 83,\n",
    "                \"width_eps_x\": 20,\n",
    "                \"depth_eps_x\": 5,\n",
    "                \"heterogeneity_x\": 10,\n",
    "                \"width_eps_z\": 20,\n",
    "                \"depth_eps_z\": 5,\n",
    "                \"heterogeneity_z\": 10,\n",
    "                \"width_u\": 20,\n",
    "                \"depth_u\": 5,\n",
    "                \"optimizer\": \"Adam\",\n",
    "                \"lr\": 0.0002,\n",
    "                \"logmin\": 1e-10,\n",
    "                \"label_smoothing\": 0.02,\n",
    "                \"callbacks\": [],\n",
    "                \"epochs\": 100,\n",
    "                \"batch_size\": 32,\n",
    "                \"tol\": 0,\n",
    "            }\n",
    "        model = RUMnet(**model_args)\n",
    "        model.instantiate()\n",
    "\n",
    "    elif model_name == \"LatenClasstMNL\": \n",
    "        model = LatentClassSimpleMNL(n_latent_classes=2, fit_method=\"mle\", optimizer=\"lbfgs\", epochs=1000, lbfgs_tolerance=1e-20)\n",
    "\n",
    "    elif model_name == \"ResLogit\": \n",
    "        model_args = {\n",
    "            \"intercept\": \"item\",\n",
    "            \"optimizer\": \"SGD\",\n",
    "            \"lr\": 1e-6,\n",
    "        }\n",
    "        model = ResLogit(n_layers=5, **model_args)\n",
    "        model.instantiate(n_items=3, n_shared_features=0, n_items_features=2)\n",
    "\n",
    "\n",
    "    elif model_name == \"NestedLogit\":\n",
    "        model = NestedLogit(optimizer=\"lbfgs\", items_nests=[[0, 2], [1]])\n",
    "\n",
    "        # Intercept for train & sm\n",
    "        model.add_coefficients(feature_name=\"intercept\", items_indexes=[0, 2])\n",
    "        \n",
    "        # betas TT and CO shared by train and sm\n",
    "        model.add_shared_coefficient(feature_name=\"travel_time\",\n",
    "                                           items_indexes=[0, 1, 2])\n",
    "        model.add_shared_coefficient(feature_name=\"cost\",\n",
    "                                           items_indexes=[0, 1, 2])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "        \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d598904f-a051-41d8-9df2-ef25e655fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_accuracies = {}\n",
    "Times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4db4b666-d99f-44dd-b150-48c357de9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models = {}\n",
    "Hists = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84ea81-c693-482e-b348-5945f8d62afe",
   "metadata": {},
   "source": [
    "## DeepHalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c8b5e88-139a-4d64-b359-f2e8c378859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(file_path, H, depth, embed, featureless=True, feature2D=False, epochs=20, batch_size=64, loss_name='nll', dropout = 0, **kwargs):\n",
    "    # build a DeepHalo choice model with the given specifications\n",
    "\n",
    "    model_fname = os.path.join(file_path, f'Depth-{depth}-H-{H}-epoch{epochs}-Fless{featureless}-2D{feature2D}-{loss_name}-embed{embed}-swiss-test.weights.h5')\n",
    "\n",
    "    \n",
    "    DeepHalo_model = DeepHaloChoiceModel(\n",
    "    H=H,\n",
    "    depth=depth,\n",
    "    embed=embed,\n",
    "    featureless=featureless,\n",
    "    feature2D=feature2D,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    loss_name=loss_name,\n",
    "    )\n",
    "    \n",
    "    return DeepHalo_model, model_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9af05e7-f8c8-45d5-95bb-c552e90fcea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load path\n",
    "file_path = os.path.join(project_root, 'Experiments', 'Real')\n",
    "\n",
    "model_args = { \n",
    "                \"depth\": 2,\n",
    "                \"H\": 20,\n",
    "                \"epochs\": 20,\n",
    "                \"featureless\": False,\n",
    "                \"feature2D\": False,\n",
    "                \"loss_name\": 'nll',\n",
    "                \"embed\": 64,\n",
    "                \"batch_size\": 64,\n",
    "                \"dropout\": 0.1,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a983b2b-287d-4dac-a7e9-9f1caafece8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 17:53:40.628198: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-31 17:53:40.628371: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-31 17:53:40.628432: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-31 17:53:41.618245: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-31 17:53:41.618381: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-31 17:53:41.618395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-12-31 17:53:41.618535: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-31 17:53:41.618572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1766 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-12-31 17:53:42.146044: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "  0%|                                                                                            | 0/20 [00:00<?, ?it/s]2025-12-31 17:53:46.061235: I external/local_xla/xla/service/service.cc:168] XLA service 0x7c7af920b3b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-31 17:53:46.061314: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-12-31 17:53:46.078633: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-31 17:53:46.122103: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767164026.187330   14769 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 19 Train Loss 0.7386: 100%|███████████████████████████████████████████████████████| 20/20 [00:30<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "avail_train = train_dataset.available_items_by_choice.astype(np.float32)\n",
    "X_train = train_dataset.items_features_by_choice[0].astype(np.float32)    \n",
    "\n",
    "# Build model and initialize it\n",
    "DeepHalo_model, model_fname = build_model(file_path, **model_args)\n",
    "_ = DeepHalo_model.deep_halo_core(X_train[:2], avail_train[:2], training=False)\n",
    "\n",
    "\n",
    "\n",
    "# Learn the model\n",
    "start = time.time()\n",
    "DeepHalo_model.fit(train_dataset)\n",
    "Times['DeepHalo'] = time.time() - start\n",
    "\n",
    "# Save/load weights\n",
    "model_filename = model_fname\n",
    "\n",
    "# DeepHalo_model.deep_halo_core.load_weights(model_fname)\n",
    "\n",
    "DeepHalo_model.deep_halo_core.save_weights(\n",
    "        os.path.join(file_path, model_filename)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f692b4bc-d7b4-4cb2-8106-3e8fe0cbe2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for DeepHalo: 0.6670\n"
     ]
    }
   ],
   "source": [
    "# Compute predicted probs\n",
    "probs_DeepHalo = DeepHalo_model.predict_probas(test_dataset).numpy()\n",
    "\n",
    "y_pred_DeepHalo = np.argmax(probs_DeepHalo, axis=1)\n",
    "DeepHalo_acc, _ = choice_accuracy(probs_DeepHalo, test_dataset.choices)\n",
    "\n",
    "print(f\"Test Accuracy for DeepHalo: {DeepHalo_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c54ae315-a4b6-4926-a36a-ca2155c861b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_accuracies['DeepHalo'] = DeepHalo_acc\n",
    "Models['DeepHalo'] = DeepHalo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862119d-5508-448f-a67c-da34fcdb4462",
   "metadata": {},
   "source": [
    "## SimpleMNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddfb68b4-6cbe-4abe-a7ba-392b3ded2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L-BFGS optimizer, setting up .fit() function\n"
     ]
    }
   ],
   "source": [
    "model_name = \"SimpleMNL\"\n",
    "model = create_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52c15111-287a-409f-9494-b50191837032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the model\n",
    "start = time.time()\n",
    "hist = model.fit(train_dataset)\n",
    "Times[model_name] = time.time() - start\n",
    "\n",
    "Models[model_name] = model\n",
    "Hists[model_name] = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14311f38-b102-47e7-8393-33dbc8d43c14",
   "metadata": {},
   "source": [
    "## Conditional Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7d93d97-02e2-4c26-bdf2-2c02787386d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L-BFGS optimizer, setting up .fit() function\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ConditionalLogit\"\n",
    "model = create_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47842dd1-6754-4e05-912c-0bd84ac7c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "hist = model.fit(train_dataset)\n",
    "Times[model_name] = time.time() - start\n",
    "\n",
    "\n",
    "Models[model_name] = model\n",
    "Hists[model_name] = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f767ac-0f3d-4933-940e-0802a6897183",
   "metadata": {},
   "source": [
    "## Halo MNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84a14ad8-d4d9-4376-842a-106cd7db9030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L-BFGS optimizer, setting up .fit() function\n"
     ]
    }
   ],
   "source": [
    "model_name = \"HaloMNL\"\n",
    "model = create_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7945945-0a4e-44a2-8923-4e4adea7b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "hist = model.fit(train_dataset)\n",
    "Times[model_name] = time.time() - start\n",
    "\n",
    "Models[model_name] = model\n",
    "Hists[model_name] = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00658f02-d3f8-4e3e-9501-dd9e358ced91",
   "metadata": {},
   "source": [
    "## LowRank Halo MNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12930997-5f34-485d-bc65-98aa2c1f0dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L-BFGS optimizer, setting up .fit() function\n"
     ]
    }
   ],
   "source": [
    "model_name = \"LowRankHaloMNL\"\n",
    "model = create_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b7a7ce9-b5f7-4a85-8b57-ae02f39700c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:L-BFGS Optimization failed.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "hist = model.fit(train_dataset)\n",
    "Times[model_name] = time.time() - start\n",
    "\n",
    "Models[model_name] = model\n",
    "Hists[model_name] = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379e26f-3d34-480e-bd5a-22930f1b04d0",
   "metadata": {},
   "source": [
    "## Nested Logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba7e07a6-d2e1-459c-8109-f6ce70d94909",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_nested = load_swissmetro(preprocessing=\"biogeme_nested\")\n",
    "\n",
    "\n",
    "test_size=0.2; random_state=42; stratify=True;\n",
    "n_choices = len(dataset_nested.choices)\n",
    "    \n",
    "# Get choice indices stratified by chosen alternative\n",
    "choice_indices = np.arange(n_choices)\n",
    "\n",
    "if stratify:\n",
    "    train_idx_nested, test_idx_nested = train_test_split(\n",
    "        choice_indices, test_size=test_size, \n",
    "        random_state=random_state, stratify=dataset_nested.choices\n",
    "    )\n",
    "else:\n",
    "    train_idx_idx_nested, test_idx_nested = train_test_split(\n",
    "        choice_indices, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset_nested = dataset_nested[train_idx_nested]\n",
    "test_dataset_nested = dataset_nested[test_idx_nested]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a7946d1-30f8-4ea1-acf6-c478f5fcb3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L-BFGS optimizer, setting up .fit() function\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NestedLogit\"\n",
    "model = create_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d310a4e6-a68d-4f36-a88c-801b0a5daeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7c7bfe5b0dd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:At least one gamma value for nests is below 0.05 and is\n",
      "        clipped to 0.05 for numeric optimization purposes.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7c7bfe5b0dd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7c7bfe5b0dd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "hist = model.fit(train_dataset_nested)\n",
    "Times[model_name] = time.time() - start\n",
    "\n",
    "Models[model_name] = model\n",
    "Hists[model_name] = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d61ea-1994-45b1-86da-7c8728a85615",
   "metadata": {},
   "source": [
    "## RUMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3425720-dda9-4d46-992f-9af4979ec6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Shared Features Names were not provided, will not be able to\n",
      "                                    fit models needing them such as Conditional Logit.\n",
      "WARNING:root:Items Features Names were not provided, will not be able to\n",
      "                                fit models needing them such as Conditional Logit.\n",
      "WARNING:root:Shared Features Names were not provided, will not be able to\n",
      "                                    fit models needing them such as Conditional Logit.\n",
      "WARNING:root:Items Features Names were not provided, will not be able to\n",
      "                                fit models needing them such as Conditional Logit.\n",
      "WARNING:root:Shared Features Names were not provided, will not be able to\n",
      "                                    fit models needing them such as Conditional Logit.\n",
      "WARNING:root:Items Features Names were not provided, will not be able to\n",
      "                                fit models needing them such as Conditional Logit.\n"
     ]
    }
   ],
   "source": [
    "dataset_RUM = load_swissmetro(as_frame=False, preprocessing=\"rumnet\")\n",
    "\n",
    "\n",
    "test_size=0.2; random_state=42; stratify=True;\n",
    "n_choices = len(dataset_RUM.choices)\n",
    "    \n",
    "# Get choice indices stratified by chosen alternative\n",
    "choice_indices = np.arange(n_choices)\n",
    "\n",
    "if stratify:\n",
    "    train_idx_RUM, test_idx_RUM = train_test_split(\n",
    "        choice_indices, test_size=test_size, \n",
    "        random_state=random_state, stratify=dataset_RUM.choices\n",
    "    )\n",
    "else:\n",
    "    train_idx_RUM, test_idx_RUM = train_test_split(\n",
    "        choice_indices, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset_RUM = dataset_RUM[train_idx_RUM]\n",
    "test_dataset_RUM = dataset_RUM[test_idx_RUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2ea707f-3406-47a7-948b-bb6c89b5a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RUMnet\"\n",
    "model = create_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93092121-e1f7-4bf3-a2d5-07396a8837eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99 Train Loss 0.5668: 100%|█████████████████████████████████████████████████████| 100/100 [05:03<00:00,  3.03s/it]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "hist = model.fit(train_dataset_RUM)\n",
    "Times[model_name] = time.time() - start\n",
    "\n",
    "Models[model_name] = model\n",
    "Hists[model_name] = hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c67d3b22-4dba-4f67-ab4e-d3aa15442180",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(os.path.join(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf16b24-704f-4db0-a92c-ee983c89de7d",
   "metadata": {},
   "source": [
    "# Non-neural ML Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3fc54-59eb-4129-8a41-2ee5a45ff1c0",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da8502af-2123-491d-8603-db828589a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choice_set_to_features(dataset):\n",
    "    # Convert a ChoiceDataset into features matrix for SVM input \n",
    "\n",
    "    X_sets, y_sets = [], []\n",
    "    items_features_list = dataset.items_features_by_choice[0]  # List of tuples for ALL choices\n",
    "    avail = dataset.available_items_by_choice              # (n_choices, n_items) availability matrix\n",
    "    N = len(items_features_list)                           # Number of choice situations\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Get availability mask for a choice situation\n",
    "        available_mask = avail[i]  \n",
    "        \n",
    "        # Get all alternatives' features (even unavailable ones)\n",
    "        choice_set_feats = items_features_list[i]  \n",
    "        \n",
    "        # Create feature vector with ZEROS for unavailable items\n",
    "        set_vector = []\n",
    "        for j, alt_feats in enumerate(choice_set_feats):\n",
    "            if available_mask[j] == 1:\n",
    "                # Available: include actual features\n",
    "                set_vector.extend(alt_feats.flatten())  \n",
    "            else:\n",
    "                # Unavailable: insert zeros\n",
    "                zero_feats = np.zeros_like(alt_feats)\n",
    "                set_vector.extend(zero_feats.flatten())  \n",
    "        \n",
    "        X_sets.append(set_vector)\n",
    "        y_sets.append(dataset.choices[i])\n",
    "    \n",
    "    return np.array(X_sets), np.array(y_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70b2ad19-9f13-4b2b-a6a6-0f4aceb5f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVM data\n",
    "X_train, y_train = choice_set_to_features(train_dataset)\n",
    "X_test, y_test = choice_set_to_features(test_dataset)\n",
    "\n",
    "# Create pipeline of scaler + SVM classifier\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "# Train the SVM\n",
    "start = time.time()\n",
    "pipeline.fit(X_train, y_train)\n",
    "Times['SVM'] = time.time() - start\n",
    "\n",
    "\n",
    "Models['SVM'] = pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d122c2b-dc8d-40a2-9fa2-889ee9bcc6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy for SVM: 0.6446\n"
     ]
    }
   ],
   "source": [
    "# y_pred = pipeline.predict(X_test)\n",
    "# SVM_acc = accuracy_score(y_test, y_pred)\n",
    "pred_probas = pipeline.predict_proba(X_test)\n",
    "SVM_acc, _ = choice_accuracy(pred_probas, test_dataset.choices, test_dataset.available_items_by_choice)\n",
    "print(f\"\\nTest Accuracy for SVM: {SVM_acc:.4f}\")\n",
    "\n",
    "Test_accuracies['SVM'] = SVM_acc\n",
    "\n",
    "# # Evaluate\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred, target_names=['TRAIN', 'SM', 'CAR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26d03e6b-958a-4334-b24a-7fdc88952026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_choice_features(dataset):\n",
    "    # Create context-aware features for each choice situation\n",
    "    \n",
    "    X_ctx, y_ctx = [], []\n",
    "    items_features = dataset.items_features_by_choice[0]\n",
    "    \n",
    "    for i, choice_features_tuple in enumerate(items_features):\n",
    "        # choice_features_tuple \n",
    "        choice_set = np.array([feat.flatten() for feat in choice_features_tuple])  \n",
    "        \n",
    "        chosen_idx = int(dataset.choices[i])\n",
    "        \n",
    "        for alt_idx in range(choice_set.shape[0]):\n",
    "            alt_feats = choice_set[alt_idx]\n",
    "            \n",
    "            # CONTEXTUAL FEATURES (per our discussion):\n",
    "            set_mean = choice_set.mean(axis=0)      # Relative to set average\n",
    "            set_max = choice_set.max(axis=0)        # Relative to best competitor  \n",
    "            set_std = choice_set.std(axis=0) + 1e-8 # Competition intensity\n",
    "            \n",
    "            # 4x feature sets = 4 * n_features total\n",
    "            ctx_vector = np.concatenate([\n",
    "                alt_feats,                           # 1. Absolute quality\n",
    "                alt_feats - set_mean,                # 2. Relative to set avg\n",
    "                alt_feats - set_max,                 # 3. Relative to best\n",
    "                (alt_feats - set_mean) / set_std     # 4. Normalized relative\n",
    "            ])\n",
    "            \n",
    "            X_ctx.append(ctx_vector)\n",
    "            \n",
    "            y_ctx.append(1 if alt_idx == chosen_idx else 0)  # BINARY: chosen vs not\n",
    "    \n",
    "    return np.array(X_ctx), np.array(y_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6d74ef6-8a6c-4bd4-a554-9a7f6f5bfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contextual_to_choice_multiclass(X_ctx, y_ctx, n_items=3):\n",
    "#     \n",
    "#     # Convert binary contextual predictions back to multiclass choice labels.\n",
    "    \n",
    "\n",
    "#     n_choices = len(X_ctx) // n_items\n",
    "#     assert len(y_ctx) == n_choices * n_items, \"Data length mismatch\"\n",
    "    \n",
    "#     X_choice = []\n",
    "#     y_choice = []\n",
    "    \n",
    "#     for i in range(n_choices):\n",
    "#         # Extract one choice set (3 rows)\n",
    "#         start_idx = i * n_items\n",
    "#         end_idx = start_idx + n_items\n",
    "#         choice_set = X_ctx[start_idx:end_idx]\n",
    "        \n",
    "#         # Store choice set features\n",
    "#         X_choice.append(choice_set.mean(axis=0))  # Or concatenate, etc.\n",
    "        \n",
    "#         # Find chosen alternative index (where y_ctx=1)\n",
    "#         choice_labels = y_ctx[start_idx:end_idx]\n",
    "#         chosen_idx = np.argmax(choice_labels)  # 0,1,2 for TRAIN,SM,CAR\n",
    "        \n",
    "#         y_choice.append(chosen_idx)\n",
    "    \n",
    "#     return np.array(X_choice), np.array(y_choice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "578107ea-e7ed-44de-bede-0bffd61a1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_probs_to_choice_labels(pipeline, X_test, n_items=3, available_items_by_choice=None):\n",
    "   \n",
    "    # Convert contextual SVM probabilities to multiclass choice predictions.\n",
    "    \n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "    n_choices = len(X_test) // n_items\n",
    "    assert len(X_test) % n_items == 0, \"X_test must be multiple of n_items\"\n",
    "    \n",
    "    pred_probas = pipeline.predict_proba(X_test)[:, 1]  # Positive class probs (n_test,)\n",
    "    choice_pred = np.zeros(n_choices, dtype=int)\n",
    "    choice_probs = np.zeros((n_choices, n_items))\n",
    "    \n",
    "    for i in range(n_choices):\n",
    "        start_idx = i * n_items\n",
    "        end_idx = start_idx + n_items\n",
    "        \n",
    "        # Get probabilities for this choice set\n",
    "        set_probs = pred_probas[start_idx:end_idx]  # (n_items,)\n",
    "        \n",
    "        # Handle availability (mask unavailable alternatives)\n",
    "        if available_items_by_choice is not None:\n",
    "            avail_mask = available_items_by_choice[i]\n",
    "            set_probs[~avail_mask.astype(bool)] = 0  # Zero unavailable\n",
    "        \n",
    "        # Normalize to choice probabilities (softmax within set)\n",
    "        choice_probs[i] = set_probs / set_probs.sum()\n",
    "        \n",
    "        # Predict chosen alternative\n",
    "        choice_pred[i] = np.argmax(choice_probs[i])\n",
    "    \n",
    "    return choice_pred, choice_probs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b1ea9-91f8-4cec-a207-fd09ce75e40f",
   "metadata": {},
   "source": [
    "Contextual SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d66bbc5-a0cc-41ed-9d0b-a4e91576896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contextual data\n",
    "X_train_ctx, y_train_ctx = contextual_choice_features(train_dataset)\n",
    "X_test_ctx, y_test_ctx = contextual_choice_features(test_dataset)\n",
    "\n",
    "# Create pipeline of scaler + SVM classifier on contextual data\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "# Train the SVM\n",
    "start = time.time()\n",
    "pipeline.fit(X_train_ctx, y_train_ctx)\n",
    "Times['Context. SVM'] = time.time() - start\n",
    "\n",
    "\n",
    "Models['Context. SVM'] = pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4de18c23-4d23-4904-af18-b63c9be36cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy for SVM contextual: 0.6651\n"
     ]
    }
   ],
   "source": [
    "choice_pred, choice_probs = contextual_probs_to_choice_labels(\n",
    "    pipeline, X_test_ctx, n_items=3, \n",
    "    available_items_by_choice=test_dataset.available_items_by_choice\n",
    ")\n",
    "SVM_acc, _ = choice_accuracy(choice_probs, test_dataset.choices, test_dataset.available_items_by_choice)\n",
    "print(f\"\\nTest Accuracy for SVM contextual: {SVM_acc:.4f}\")\n",
    "\n",
    "Test_accuracies['Context. SVM'] = SVM_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef68a4-833e-4d68-b8d9-3298d482651f",
   "metadata": {},
   "source": [
    "## Ranom Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1af403a1-bc8a-471c-9c8f-629d78faf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RF pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Optional for trees\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "Times['RandomForest'] = time.time() - start\n",
    "\n",
    "\n",
    "Models['RandomForest'] = rf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88c5313f-2774-40dc-bb14-7d1ab3d3de73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy for Random Forest: 0.6451\n"
     ]
    }
   ],
   "source": [
    "# rf_pred = np.argmax(rf_pipeline.predict_proba(X_test), axis=1)\n",
    "# rf_accuracy = np.mean(rf_pred == y_test)\n",
    "\n",
    "pred_probas = rf_pipeline.predict_proba(X_test)\n",
    "rf_acc, _ = choice_accuracy(pred_probas, test_dataset.choices, test_dataset.available_items_by_choice)\n",
    "print(f\"\\nTest Accuracy for Random Forest: {rf_acc:.4f}\")\n",
    "\n",
    "Test_accuracies['RandomForest'] = rf_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9fcfc8-5412-46ff-a9a7-f26ee7bf5a8b",
   "metadata": {},
   "source": [
    "Contextual Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4bc09e7-9538-4242-b381-254cd703ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Optional for trees\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "rf_pipeline.fit(X_train_ctx, y_train_ctx)\n",
    "Times['Context. RandomForest'] = time.time() - start\n",
    "\n",
    "\n",
    "Models['Context. RandomForest'] = rf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e82e4d0-dfa9-4d5c-89b4-9d72a70e9d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy for SVM contextual: 0.6936\n"
     ]
    }
   ],
   "source": [
    "choice_pred, choice_probs = contextual_probs_to_choice_labels(\n",
    "    rf_pipeline, X_test_ctx, n_items=3, \n",
    "    available_items_by_choice=test_dataset.available_items_by_choice\n",
    ")\n",
    "rf_acc, _ = choice_accuracy(choice_probs, test_dataset.choices, test_dataset.available_items_by_choice)\n",
    "print(f\"\\nTest Accuracy for SVM contextual: {rf_acc:.4f}\")\n",
    "\n",
    "Test_accuracies['Context. RandomForest'] = rf_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea03bd83-33f2-4147-91da-ad888b59750d",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54cd1254-a33f-48c7-9c76-575fe347f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tf/lib/python3.10/site-packages/xgboost/training.py:199: UserWarning: [18:02:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# XGBoost \n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Optional\n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=2,  # Handle imbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# LightGBM \n",
    "lgb_pipeline = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train & predict\n",
    "start = time.time()\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "Times['XGB'] = time.time() - start\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "lgb_pipeline.fit(X_train, y_train)\n",
    "Times['LGB'] = time.time() - start\n",
    "\n",
    "\n",
    "\n",
    "Models['XGB'] = xgb_pipeline\n",
    "Models['LGB'] = lgb_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f562adc-a66a-480f-9fdb-5ec93194baaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy for LGB: 0.6968\n",
      "\n",
      "Test Accuracy for XGB: 0.7318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pred_probas = lgb_pipeline.predict_proba(X_test)\n",
    "lgb_acc, _ = choice_accuracy(pred_probas, test_dataset.choices, test_dataset.available_items_by_choice)\n",
    "print(f\"\\nTest Accuracy for LGB: {lgb_acc:.4f}\")\n",
    "\n",
    "\n",
    "pred_probas = xgb_pipeline.predict_proba(X_test)\n",
    "xgb_acc, _ = choice_accuracy(pred_probas, test_dataset.choices, test_dataset.available_items_by_choice)\n",
    "print(f\"\\nTest Accuracy for XGB: {xgb_acc:.4f}\")\n",
    "\n",
    "\n",
    "Test_accuracies['LGB'] = lgb_acc\n",
    "Test_accuracies['XGB'] = xgb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da1551-9089-48d3-a25d-cf661bb9b759",
   "metadata": {},
   "source": [
    "Contextual gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f464c03-9b7e-4ada-90cb-26f4b2eb49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost (Top choice)\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Optional\n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=2,  # Handle imbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# LightGBM (Fastest)\n",
    "lgb_pipeline = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train & predict\n",
    "start = time.time()\n",
    "lgb_pipeline.fit(X_train_ctx, y_train_ctx)\n",
    "Times['Context. LGB'] = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "xgb_pipeline.fit(X_train_ctx, y_train_ctx)\n",
    "Times['Context. XGB'] = time.time() - start\n",
    "\n",
    "\n",
    "\n",
    "Models['Context. XGB'] = xgb_pipeline\n",
    "Models['Context. LGB'] = lgb_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dff042ae-28ee-4182-9b22-d481a08c177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy for SVM contextual: 0.7024\n",
      "\n",
      "Test Accuracy for SVM contextual: 0.7038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "choice_pred, choice_probs = contextual_probs_to_choice_labels(\n",
    "    xgb_pipeline, X_test_ctx, n_items=3, \n",
    "    available_items_by_choice=test_dataset.available_items_by_choice\n",
    ")\n",
    "xgb_acc, _ = choice_accuracy(choice_probs, test_dataset.choices, test_dataset.available_items_by_choice)\n",
    "print(f\"\\nTest Accuracy for SVM contextual: {xgb_acc:.4f}\")\n",
    "\n",
    "Test_accuracies['Context. XGB'] = xgb_acc\n",
    "\n",
    "choice_pred, choice_probs = contextual_probs_to_choice_labels(\n",
    "    lgb_pipeline, X_test_ctx, n_items=3, \n",
    "    available_items_by_choice=test_dataset.available_items_by_choice\n",
    ")\n",
    "lgb_acc, _ = choice_accuracy(choice_probs, test_dataset.choices, test_dataset.available_items_by_choice)\n",
    "print(f\"\\nTest Accuracy for SVM contextual: {lgb_acc:.4f}\")\n",
    "\n",
    "Test_accuracies['Context. LGB'] = lgb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8825e6-d568-46d2-8d66-ddb61b9fbaf9",
   "metadata": {},
   "source": [
    "# Evaluate test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8fa20cd-5594-4026-91a1-ea92252d73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_keys = list(set(Models.keys()) - set(Test_accuracies.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "012eada7-ae36-4a18-83f3-998427e35fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function ChoiceModel.batch_predict at 0x7c7ad84aecb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function ChoiceModel.batch_predict at 0x7c7ad84aecb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:root:At least one gamma value for nests is below 0.05 and is\n",
      "        clipped to 0.05 for numeric optimization purposes.\n"
     ]
    }
   ],
   "source": [
    "for model_name in missing_keys:\n",
    "    if model_name == \"NestedLogit\":\n",
    "        dataset_test = test_dataset_nested\n",
    "    elif model_name == \"RUMnet\":\n",
    "        dataset_test = test_dataset_RUM\n",
    "    else:\n",
    "        dataset_test = test_dataset\n",
    "        \n",
    "    model = Models[model_name]\n",
    "    pred_probas = model.predict_probas(dataset_test).numpy()\n",
    "    test_acc, _ = choice_accuracy(pred_probas, dataset_test.choices, dataset_test.available_items_by_choice)\n",
    "    Test_accuracies[model_name] = test_acc\n",
    "    \n",
    "    # print(f\"Test Accuracy for {model_name}: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f14945bc-84fb-494b-abde-dceb237277da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DeepHalo</th>\n",
       "      <td>0.666978</td>\n",
       "      <td>30.714309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.644590</td>\n",
       "      <td>10.905960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Context. SVM</th>\n",
       "      <td>0.665112</td>\n",
       "      <td>67.586082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.645056</td>\n",
       "      <td>1.274715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Context. RandomForest</th>\n",
       "      <td>0.693563</td>\n",
       "      <td>2.976948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGB</th>\n",
       "      <td>0.696828</td>\n",
       "      <td>0.961082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>0.731810</td>\n",
       "      <td>1.507641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Context. XGB</th>\n",
       "      <td>0.702425</td>\n",
       "      <td>0.792533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Context. LGB</th>\n",
       "      <td>0.703825</td>\n",
       "      <td>0.645908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RUMnet</th>\n",
       "      <td>0.736007</td>\n",
       "      <td>303.063342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LowRankHaloMNL</th>\n",
       "      <td>0.606343</td>\n",
       "      <td>40.993907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimpleMNL</th>\n",
       "      <td>0.617537</td>\n",
       "      <td>4.593346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HaloMNL</th>\n",
       "      <td>0.606343</td>\n",
       "      <td>4.541348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConditionalLogit</th>\n",
       "      <td>0.608209</td>\n",
       "      <td>19.027026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NestedLogit</th>\n",
       "      <td>0.677991</td>\n",
       "      <td>14.243422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Test Accuracy        Time\n",
       "DeepHalo                    0.666978   30.714309\n",
       "SVM                         0.644590   10.905960\n",
       "Context. SVM                0.665112   67.586082\n",
       "RandomForest                0.645056    1.274715\n",
       "Context. RandomForest       0.693563    2.976948\n",
       "LGB                         0.696828    0.961082\n",
       "XGB                         0.731810    1.507641\n",
       "Context. XGB                0.702425    0.792533\n",
       "Context. LGB                0.703825    0.645908\n",
       "RUMnet                      0.736007  303.063342\n",
       "LowRankHaloMNL              0.606343   40.993907\n",
       "SimpleMNL                   0.617537    4.593346\n",
       "HaloMNL                     0.606343    4.541348\n",
       "ConditionalLogit            0.608209   19.027026\n",
       "NestedLogit                 0.677991   14.243422"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Test Accuracy': Test_accuracies,\n",
    "    'Time': Times\n",
    "})\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a192efac-b7fa-4ea0-a9fa-efc6d060822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_names = [key for key in Models.keys() if key != 'RUMnet' and key != 'DeepHalo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4734d389-5dfb-4ae0-9eca-01913c7cd5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/Q3/DeepHalo-tf/Final/DeepHalo/Experiments/Real/Accuracies.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump([Models[name] for name in saved_model_names], os.path.join(file_path,\"choice_models.pkl\") )  \n",
    "joblib.dump(Times, os.path.join(file_path,\"Times.pkl\") ) \n",
    "joblib.dump(Test_accuracies, os.path.join(file_path,\"Accuracies.pkl\") )  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu-2.15)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
